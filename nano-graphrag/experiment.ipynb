{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f572ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: jhgan/ko-sroberta-multitask\n",
      "INFO:nano-graphrag:Load KV full_docs with 10 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 10 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 43 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 22 data\n",
      "INFO:nano-graphrag:Loaded graph from ./graphrag/graph_chunk_entity_relation.graphml with 732 nodes, 161 edges\n",
      "INFO:nano-vectordb:Load (724, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './graphrag/vdb_entities.json'} 724 data\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.61it/s]\n",
      "INFO:nano-graphrag:Using 20 entites, 5 communities, 38 relations, 9 text units\n",
      "INFO:nano-graphrag:Revtrieved 22 communities\n",
      "INFO:nano-graphrag:Grouping to 1 groups for global search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Local RAG Recommendation ===\n",
      "## ë‘¥ê·¼ ì–¼êµ´í˜•ê³¼ ì§ëª¨ì˜ íŠ¹ì„±\n",
      "\n",
      "ë‘¥ê·¼ ì–¼êµ´í˜•ì€ ë¶€ë“œëŸ¬ìš´ ìœ¤ê³½ì„ ê³¼ í’ì„±í•œ ë³¼ë¡œ íŠ¹ì§•ì§€ì–´ì§€ë©°, ì´ëŸ¬í•œ ì–¼êµ´í˜•ì— ì í•©í•œ ìŠ¤íƒ€ì¼ì€ ì–¼êµ´ì˜ êµ´ê³¡ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì§ëª¨ì™€ êµµì€ ëª¨ë°œì„ ê°€ì§„ ê²½ìš°, ë³´ë‹¤ ì •ëˆëœ ëŠë‚Œì˜ ìŠ¤íƒ€ì¼ì´ ì í•©í•  ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¡°í•©ì„ ê³ ë ¤í•  ë•Œ, ê´€ë¦¬ ë‚œì´ë„ê°€ ì‰¬ìš´ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ì¶”ì²œ ìŠ¤íƒ€ì¼: ê°€ë¥´ë§ˆíŒ\n",
      "\n",
      "ê°€ë¥´ë§ˆíŒì€ ë‘¥ê·¼ ì–¼êµ´í˜•ì— ì˜ ì–´ìš¸ë¦¬ëŠ” ë‚¨ì„± ìŠ¤íƒ€ì¼ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì€ ê¹”ë”í•˜ê³  ì„¸ë ¨ëœ ì¸ìƒì„ ì£¼ë©°, íŠ¹íˆ ì§ëª¨ì™€ êµµì€ ëª¨ë°œì— ì í•©í•©ë‹ˆë‹¤. **ê°€ë¥´ë§ˆíŒ**ì˜ íŠ¹ì§•ì€ ê°ì§„ ì–¼êµ´í˜•ì„ ë¶€ë“œëŸ½ê²Œ ì»¤ë²„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë‘¥ê·¼ ì–¼êµ´í˜•ì—ë„ ë§¤ìš° ë§¤ë ¥ì ì…ë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì€ ë³¼ë¥¨ì´ ë¶€ì¡±í•œ ê²½ìš°ì—ë„ íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê³ ê°ì˜ ì–¼êµ´í˜•ì— ë§ê²Œ ê°œì¸ ë§ì¶¤í˜• ì‹œìˆ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŠ¹íˆ, ë©´ì ‘ì´ë‚˜ ì†Œê°œíŒ… ë“±ì—ì„œ ê¸ì •ì ì¸ ì¸ìƒì„ ì¤„ ìˆ˜ ìˆëŠ” ìŠ¤íƒ€ì¼ë¡œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
      "\n",
      "## ì¶”ê°€ ìŠ¤íƒ€ì¼ë§: ê°€ì¼ì»·\n",
      "\n",
      "ë˜ ë‹¤ë¥¸ ì¶”ì²œ ìŠ¤íƒ€ì¼ì€ **ê°€ì¼ì»·**ì…ë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì€ ì§ëª¨ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ìì—°ìŠ¤ëŸ½ê³  ë‹¨ì •í•œ ëŠë‚Œì„ ì¤ë‹ˆë‹¤. ê´€ë¦¬ê°€ ì‰¬ìš´ ìŠ¤íƒ€ì¼ì´ë©°, ì´ë§ˆë¥¼ ë“œëŸ¬ë‚´ì–´ ì„±ìˆ™í•˜ê³  ê¹”ë”í•œ ë¶„ìœ„ê¸°ë¥¼ ì—°ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¼ì»·ì€ ë‹¤ì–‘í•œ ì–¼êµ´í˜•ì— ì–´ìš¸ë¦¬ì§€ë§Œ, íŠ¹íˆ ë‘¥ê·¼ ì–¼êµ´í˜•ì—ê²Œ ë” ì˜ ë§ê¸° ë•Œë¬¸ì— ì¢‹ìŠµë‹ˆë‹¤. ê°€ì¼ì»·ì€ ì•ë¨¸ë¦¬ì˜ ê¸¸ì´ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆì–´, ê°œì¸ì˜ ì·¨í–¥ì— ë§ê²Œ ìŠ¤íƒ€ì¼ë§í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ê´€ë¦¬ì˜ ìš©ì´í•¨\n",
      "\n",
      "ê°€ë¥´ë§ˆíŒê³¼ ê°€ì¼ì»· ëª¨ë‘ ê´€ë¦¬ê°€ ìš©ì´í•˜ë©°, ë°”ìœ í˜„ëŒ€ì¸ë“¤ì—ê²Œ íŠ¹íˆ ì í•©í•œ ì„ íƒì…ë‹ˆë‹¤. ìŠ¤íƒ€ì¼ë§ì— í•„ìš”í•œ ì œí’ˆ, ì˜ˆë¥¼ ë“¤ì–´ ê°€ë²¼ìš´ ì™ìŠ¤ë‚˜ í—¤ì–´ ìŠ¤í”„ë ˆì´ë¥¼ ì´ìš©í•˜ë©´ ì‰½ê²Œ ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìŠ¤íƒ€ì¼ë“¤ì€ ì‹œê°„ì˜ ì œì•½ì´ ìˆëŠ” ë‚¨ì„±ë“¤ì—ê²Œ ë§¤ìš° ë°”ëŒì§í•œ ì˜µì…˜ì´ ë  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ìœ„ì˜ ìŠ¤íƒ€ì¼ ì¡°í•©ì„ í†µí•´ ë‘¥ê·¼ ì–¼êµ´í˜•ê³¼ ì§ëª¨, êµµì€ ëª¨ë°œì—ì„œ ë§¤ë ¥ì ì¸ ì´ë¯¸ì§€ë¥¼ ì—°ì¶œí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ê³  ì´ì™€ ì í•©í•œ ê´€ë¦¬ë²•ì„ í†µí•´ ìì‹ ë§Œì˜ ë©‹ì§„ ë£©ì„ ë§Œë“¤ì–´ ë³´ì„¸ìš”!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global RAG Recommendation ===\n",
      "## ì¶”ì²œ ìŠ¤íƒ€ì¼\n",
      "\n",
      "ë‹¹ì‹ ì˜ ë‘¥ê·¼ ì–¼êµ´í˜•ê³¼ ì§ëª¨, êµµì€ ëª¨ë°œì— ì í•©í•œ ìŠ¤íƒ€ì¼ì„ ê³ ë ¤í•  ë•Œ, ë‘ ê°€ì§€ ìŠ¤íƒ€ì¼ì´ íŠ¹íˆ ì¶”ì²œë©ë‹ˆë‹¤: **ê°€ë¥´ë§ˆíŒ**ê³¼ **ê°€ì¼ì»·**ì…ë‹ˆë‹¤.\n",
      "\n",
      "### ê°€ë¥´ë§ˆíŒ\n",
      "\n",
      "ê°€ë¥´ë§ˆíŒì€ ê°ì§„ ì–¼êµ´í˜•ê³¼ ë‘¥ê·¼ ì–¼êµ´í˜• ëª¨ë‘ì— ì í•©í•˜ì—¬, ë¶€ë“œëŸ¬ìš´ ì¸ìƒì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì€ ì§ëª¨ë¥¼ ê°€ì§„ ë‚¨ì„±ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë³¼ë¥¨ì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ë•Œë¬¸ì— ë‘¥ê·¼ ì–¼êµ´í˜•ì„ ë”ìš± ë§¤ë ¥ì ìœ¼ë¡œ ë³´ì´ê²Œ í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê´€ë¦¬ê°€ ìš©ì´í•œ ì¥ì ì´ ìˆì–´ ë°”ìœ ì¼ìƒ ì†ì—ì„œë„ í¸ë¦¬í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ê°€ì¼ì»·\n",
      "\n",
      "ê°€ì¼ì»·ì€ êµµì€ ëª¨ë°œì— ì˜ ì–´ìš¸ë¦¬ë©°, ê¹”ë”í•œ ëŠë‚Œì„ ì£¼ëŠ” ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤. ì´ ìŠ¤íƒ€ì¼ì€ ì–¼êµ´í˜•ì„ ë³´ì™„í•˜ê³ , ì‹œì›í•œ ì¸ìƒì„ ì œê³µí•˜ë©° ìœ ì§€ ê´€ë¦¬ê°€ ì‰½ê¸° ë•Œë¬¸ì— ë‹¹ì‹ ì—ê²Œ ì í•©í•  ê²ƒì…ë‹ˆë‹¤. êµµì€ ëª¨ë°œì„ ê°€ì§„ ê²½ìš°, ì´ëŸ¬í•œ ìŠ¤íƒ€ì¼ë§ì€ ë”ìš± ëšœë ·í•œ ì¸ìƒì„ ë‚¨ê¸°ëŠ”ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## ê²°ë¡ \n",
      "\n",
      "ê²°ë¡ ì ìœ¼ë¡œ, ë‘¥ê·¼ ì–¼êµ´í˜•ê³¼ êµµì€ ì§ëª¨ë¥¼ ê°€ì§„ ë‹¹ì‹ ì—ê²ŒëŠ” ê°€ë¥´ë§ˆíŒê³¼ ê°€ì¼ì»·ì´ ì¢‹ì€ ì„ íƒì´ ë  ê²ƒì…ë‹ˆë‹¤. ì´ ë‘ ìŠ¤íƒ€ì¼ ëª¨ë‘ ê´€ë¦¬ê°€ ìš©ì´í•˜ë©°, ê°ê° íŠ¹ìœ ì˜ ë§¤ë ¥ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì˜µì…˜ì„ì„ ê³ ë ¤í•˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ê¸°ì— ì•ì„œ, ê° ìŠ¤íƒ€ì¼ì˜ ì‹œê°ì  íš¨ê³¼ì™€ í¸ì˜ì„±ì„ ì˜ ë¹„êµí•´ ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nano_graphrag import GraphRAG\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env íŒŒì¼ì— OPENAI_API_KEYê°€ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "from nano_graphrag._llm import openai_complete_if_cache\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nano_graphrag import GraphRAG, QueryParam  # QueryParam ë³„ë„ import\n",
    "\n",
    "\n",
    "async def my_custom_llm(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    return await openai_complete_if_cache(\n",
    "        \"gpt-4o-mini\",  # ì›í•˜ëŠ” ëª¨ë¸\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "# ì•ˆì •ì ìœ¼ë¡œ ë¡œë”© ê°€ëŠ¥í•œ í•œêµ­ì–´ SBERT ëª¨ë¸\n",
    "model_name = 'jhgan/ko-sroberta-multitask'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# nano-graphragì—ì„œ ìš”êµ¬í•˜ëŠ” ë˜í•‘\n",
    "def wrap_embedding_func_with_attrs(embedding_dim, max_token_size):\n",
    "    def decorator(func):\n",
    "        func.embedding_dim = embedding_dim\n",
    "        func.max_token_size = max_token_size\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=512)\n",
    "async def local_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ SBERT ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True)\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./graphrag\",\n",
    "    best_model_func=my_custom_llm,\n",
    "    embedding_func=local_embedding_func,\n",
    "\n",
    "    # âœ… ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ ì•„ì£¼ í¬ê²Œ ì„¤ì •í•´ì„œ \"ì ˆëŒ€ ìë¥´ì§€ ì•Šê²Œ\" í•¨\n",
    "    chunk_token_size=100000,\n",
    "    chunk_overlap_token_size=0,\n",
    ")\n",
    "\n",
    "# 1) ì§ˆì˜ ë‚´ìš© ì •ì˜\n",
    "query_text = (\n",
    "    \"ë‚˜ëŠ” ë‘¥ê·¼í˜• ì–¼êµ´ì´ê³ , ì§ëª¨ì— êµµì€ ëª¨ë°œì„ ê°€ì¡Œìœ¼ë©°, \"\n",
    "    \"ê´€ë¦¬ ë‚œì´ë„ ì‰¬ìš´ ë‚¨ì„± ìŠ¤íƒ€ì¼ì„ ì¶”ì²œí•´ì¤˜.\"\n",
    ")\n",
    "\n",
    "param_local = QueryParam(mode=\"local\")\n",
    "response_local = graph_func.query(query_text, param_local)\n",
    "print(\"=== Local RAG Recommendation ===\")\n",
    "print(response_local)  # JSON í˜•ì‹ì˜ community_report_hair ê²°ê³¼\n",
    "\n",
    "param_global = QueryParam(mode=\"global\")\n",
    "response_global = graph_func.query(query_text, param_global)\n",
    "print(\"=== Global RAG Recommendation ===\")\n",
    "print(response_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0067284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv('hair_data/for_graph.csv')\n",
    "df\n",
    "# ìš”ì•½ ë¶€ë¶„ì— ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¦‰ *,# ë“±ì´ ìˆìœ¼ë©´ ì‚­ì œí•˜ëŠ” ì½”ë“œ\n",
    "import re\n",
    "def remove_markdown(text):\n",
    "    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìë¥¼ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì œê±°\n",
    "    text = re.sub(r'[*#]', '', text)\n",
    "    return text\n",
    "df['ìš”ì•½'] = df['ìš”ì•½'].apply(remove_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce4f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 10 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 10 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 1(10%) chunks,  10 entities(duplicated), 8 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 2(20%) chunks,  21 entities(duplicated), 18 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 3(30%) chunks,  44 entities(duplicated), 34 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 4(40%) chunks,  62 entities(duplicated), 48 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ´ Processed 5(50%) chunks,  85 entities(duplicated), 67 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processed 6(60%) chunks,  106 entities(duplicated), 87 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 7(70%) chunks,  129 entities(duplicated), 107 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‡ Processed 8(80%) chunks,  150 entities(duplicated), 127 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 9(90%) chunks,  190 entities(duplicated), 151 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 10(100%) chunks,  746 entities(duplicated), 161 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Inserting 724 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.17it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.12it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.19it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.61it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.94it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.00it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.04it/s]\n",
      "INFO:nano-graphrag:[Community Report]...\n",
      "INFO:nano-graphrag:Each level has communities: {0: 9, 1: 13}\n",
      "INFO:nano-graphrag:Generating by levels: [1, 0]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 1 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 2 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 3 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 4 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ´ Processed 5 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processed 6 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 7 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‡ Processed 8 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 9 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 10 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 11 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 12 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processed 13 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processed 14 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ´ Processed 15 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processed 16 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processed 17 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‡ Processed 18 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â  Processed 19 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Processed 20 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processed 21 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¹ Processed 22 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Writing graph with 732 nodes, 161 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"ìš”ì•½\"].dropna().astype(str).tolist()  # ğŸ”¥ í–‰ í•˜ë‚˜ = í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì‚¬ìš©ë¨\n",
    "graph_func.insert(docs[25:35])\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c3b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.03it/s]\n",
      "INFO:nano-graphrag:Using 20 entites, 1 communities, 18 relations, 7 text units\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Local RAG ëª¨ë“œ\u001b[39;00m\n\u001b[1;32m     10\u001b[0m param_local \u001b[38;5;241m=\u001b[39m QueryParam(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m response_local \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Local RAG Recommendation ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_local)  \u001b[38;5;66;03m# JSON í˜•ì‹ì˜ community_report_hair ê²°ê³¼\u001b[39;00m\n",
      "File \u001b[0;32m~/AIworkspace/poject/DSCD-1/nano-graphrag/nano_graphrag/graphrag.py:224\u001b[0m, in \u001b[0;36mGraphRAG.query\u001b[0;34m(self, query, param)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, param: QueryParam \u001b[38;5;241m=\u001b[39m QueryParam()):\n\u001b[1;32m    223\u001b[0m     loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam  # QueryParam ë³„ë„ import\n",
    "\n",
    "# 1) ì§ˆì˜ ë‚´ìš© ì •ì˜\n",
    "query_text = (\n",
    "    \"ë‚˜ëŠ” ë‘¥ê·¼í˜• ì–¼êµ´ì´ê³ , ì§ëª¨ì— êµµì€ ëª¨ë°œì„ ê°€ì¡Œìœ¼ë©°, \"\n",
    "    \"ê´€ë¦¬ ë‚œì´ë„ ì‰¬ìš´ ë‚¨ì„± ìŠ¤íƒ€ì¼ì„ ì¶”ì²œí•´ì¤˜.\"\n",
    ")\n",
    "\n",
    "# 2) Local RAG ëª¨ë“œ\n",
    "param_local = QueryParam(mode=\"local\")\n",
    "response_local = graph_func.query(query_text, param_local)\n",
    "print(\"=== Local RAG Recommendation ===\")\n",
    "print(response_local)  # JSON í˜•ì‹ì˜ community_report_hair ê²°ê³¼\n",
    "\n",
    "# 3) Global RAG ëª¨ë“œ\n",
    "param_global = QueryParam(mode=\"global\")\n",
    "response_global = graph_func.query(query_text, param_global)\n",
    "print(\"=== Global RAG Recommendation ===\")\n",
    "print(response_global)\n",
    "\n",
    "# 4) (Optional) Naive ëª¨ë“œ\n",
    "# param_naive = QueryParam(mode=\"naive\")\n",
    "# response_naive = graph_func.query(query_text, param_naive)\n",
    "# print(\"=== Naive RAG Recommendation ===\")\n",
    "# print(response_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
